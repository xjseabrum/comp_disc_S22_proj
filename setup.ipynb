{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "setup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIVRetXHkWrTXekVOWod5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjseabrum/comp_disc_S22_proj/blob/main/setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRYhnkIEZzO0"
      },
      "outputs": [],
      "source": [
        "import os as os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/MyDrive/comp_proj\")"
      ],
      "metadata": {
        "id": "btSrKAGuZ34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install -U numpy\n",
        "!pip install kneed"
      ],
      "metadata": {
        "id": "0jzZMDQmZ5F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Python interface to the Penn Discourse Treebank 2\n",
        "\n",
        "!git clone https://github.com/cgpotts/pdtb2.git"
      ],
      "metadata": {
        "id": "Hr4QsUpgZ6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "wRf8mJRSZ8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# locate to the project dir\n",
        "%cd /content/gdrive/MyDrive/comp_proj/pdtb2/"
      ],
      "metadata": {
        "id": "TNuTwcesZ9iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the csv file\n",
        "!unzip pdtb2.csv.zip"
      ],
      "metadata": {
        "id": "5oWlE_ZrZ_LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 __init__.py"
      ],
      "metadata": {
        "id": "bFHbiED6aAcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import statements\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from pdtb2 import CorpusReader, Datum\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "a68lcn5VaCu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction and Preprocessing"
      ],
      "metadata": {
        "id": "WUpjjRYMaHMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attribute reference http://compprag.christopherpotts.net/swda.html\n",
        "iterator = CorpusReader('pdtb2.csv').iter_data(display_progress=False)\n",
        "for _ in range(17): next(iterator)\n",
        "d = next(iterator)\n",
        "d.Relation"
      ],
      "metadata": {
        "id": "dtsUYPhkaH9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "evL0iKoqaJvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in Data\n",
        "X_sentences = []\n",
        "y = []\n",
        "rel_tag = []\n",
        "iterator = CorpusReader('pdtb2.csv').iter_data(display_progress=False)\n",
        "try:\n",
        "    while(True):\n",
        "        d = next(iterator)\n",
        "        arg1 = d.Arg2_RawText.lower()\n",
        "        arg2 = d.Arg2_Attribution_RawText.lower()\n",
        "        relation = d.Relation.lower()\n",
        "        connection = None\n",
        "        if relation == \"Explicit\":\n",
        "            connection = d.ConnHead\n",
        "            X_sentences.append([d.Arg1_RawText,d.ConnHead,d.Arg2_RawText])\n",
        "            y.append(d.ConnHead)\n",
        "            rel_tag.append(d.ConnHeadSemClass1)\n",
        "except:\n",
        "    print(\"end\")"
      ],
      "metadata": {
        "id": "d9QvAxJWaLC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arg1 sentence and arg2 sentence\n",
        "X_sentences[0]"
      ],
      "metadata": {
        "id": "OeVKPmyoaMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connective phrase\n",
        "y[0]"
      ],
      "metadata": {
        "id": "oZymbouvaOjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_sentences)"
      ],
      "metadata": {
        "id": "kfaqTAZEaPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features using bert sentence transformers and concatenate two embedings.\n",
        "# reference : https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens\n",
        "# save features to file because it cost a lot of time extracting the features\n",
        "\n",
        "n = len(X_sentences)\n",
        "with open('features.npy', 'wb') as f:\n",
        "  c = 0\n",
        "  start = time.time()\n",
        "  for sentences in X_sentences:\n",
        "    feature = np.concatenate(model.encode(sentences))\n",
        "    np.save(f, feature)\n",
        "    if c % 1000 == 0:\n",
        "      print(str(c) + \"/\" + str(n) + \" complete.\")\n",
        "    if c % n == 0:\n",
        "      print(str(c) + \"/\" + str(n) + \" complete.\")\n",
        "    c += 1\n",
        "  end = time.time()\n",
        "  print(\"Total runtime: {} seconds\".format(round(end - start, 2)))"
      ],
      "metadata": {
        "id": "ZOfKI9P2aQ7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view_labels(X, X_sentences, algo_labels, u):\n",
        "  # algo_labels is labels as found by the algorithm in 1D array form\n",
        "  # u is number of clusters\n",
        "\n",
        "  arg1s = [x[0] for x in X_sentences]\n",
        "  arg2s = [x[2] for x in X_sentences]\n",
        "  arg_conn = [x[1] for x in X_sentences]\n",
        "  arg_label = [y for y in rel_tag]\n",
        "  df = pd.DataFrame([arg1s,arg2s,arg_conn,algo_labels, arg_label]).transpose()\n",
        "  df.columns = ['arg1','arg2','connective', 'algo', 'pdtb_tag']\n",
        "\n",
        "  df[['primary', 'secondary', 'tertiary']] = df.pdtb_tag.str.split('.', expand=True)\n",
        "\n",
        "  labels = []\n",
        "  for label in range(u):\n",
        "    df2 = df[df['algo'] == label]\n",
        "    tmp_list1 = set(df2[\"connective\"])\n",
        "    [x, y] = [label, [tmp_list1]]\n",
        "    labels.append([x,y])\n",
        "\n",
        "  connective_to_label = []\n",
        "  for word in range(len(list(set(df[\"connective\"])))):\n",
        "    connective = list(set(df[\"connective\"]))[word] \n",
        "    df2 = df[df[\"connective\"] == connective]\n",
        "    tmp_list2 = set(df2[\"algo\"])\n",
        "    [x,y] = [connective, [tmp_list2]]\n",
        "    connective_to_label.append([x,y])\n",
        "\n",
        "  return df, labels, connective_to_label"
      ],
      "metadata": {
        "id": "OZI_kuYSj4my"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}